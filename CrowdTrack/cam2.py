from flask import Blueprint, render_template, Response, jsonify, request, render_template_string
import cv2
import os
import threading
import torch
import time
import numpy as np
from ultralytics import RTDETR
from collections import deque, defaultdict

cam2_bp = Blueprint('cam2', __name__)

# --- YOLO ëª¨ë¸ (ì‚¬ëŒ íƒì§€ ì „ìš©) ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ğŸ” Using device: {device}")
DEVICE_ARG = 0 if device == 'cuda' else 'cpu'

# ëª¨ë¸ì„ ì§€ì—° ë¡œë”©
model = None

def get_model():
    global model
    if model is None:
        try:
            model = RTDETR("rtdetr-l.pt")
            # model = YOLO("yolo11s.pt", task="detect")
        except Exception as e:
            print(f"Model loading error: {e}")
            return None
    return model

def infer(img, **kwargs):
    m = get_model()
    if m is None:
        return [type('MockResult', (), {'boxes': None, 'plot': lambda img=img: img})()]
    return m.predict(img, device=DEVICE_ARG, imgsz=416, classes=[0], conf=0.25, iou=0.45, max_det=100, 
                     half=True, augment=False, verbose=False **kwargs)

# --- ì˜ìƒ ê²½ë¡œ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
video_path = os.path.join(BASE_DIR, "static", "videos", "buchoen_station_square_2_30s.mp4")

# --- ì „ì—­ ë³€ìˆ˜: êµ¬ì—­ë³„ ê°ì²´ íƒì§€ ê²°ê³¼ (MAIN ì¹´ë©”ë¼ìš©ì´ì§€ë§Œ API í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€) ---
zone_detections = {1: 0, 2: 0, 3: 0, 4: 0}
detection_lock = threading.Lock()

# --- ê²½ë³´ ì„ê³„ê°’ (API í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€) ---
WARNING_THRESHOLD = 50
DANGER_THRESHOLD = 50

# --- CAM2 ì´ë²¤íŠ¸ ê°ì§€ ì„¤ì • ---
cam2_zone_coords = {
    "running_zone": [(960, 540), (2000, 540), (2000, 1500), (960, 1500)],
}

# --- ì´ë²¤íŠ¸ ê°ì§€ ì„¤ì • ---
HISTORY_LEN = 12
SPEED_WINDOW = 3
TRAIL_LEN = 6
SPEED_THRESHOLD_GENERAL = 2
track_history = defaultdict(lambda: deque(maxlen=HISTORY_LEN))
event_detections = {"all_event_persons": set(), "all_event_count": 0}

# --- CAM2 ì´ë²¤íŠ¸ ê°ì§€ í•¨ìˆ˜ë“¤ ---
def is_valid_person_detection(x1, y1, x2, y2, conf):
    """ì‚¬ëŒ ê°ì§€ í›„ì²˜ë¦¬ ê²€ì¦"""
    w, h = x2 - x1, y2 - y1
    if w > 180 or h > 180:
        return False
    return True

def create_roi_mask(frame, roi_polygon):
    """ROI ë§ˆìŠ¤í¬ ìƒì„±"""
    pts = np.array(roi_polygon, np.int32)
    cv2.polylines(frame, [pts], isClosed=True, color=(255,255,0), thickness=2)
    mask = np.zeros(frame.shape[:2], dtype=np.uint8)
    cv2.fillPoly(mask, [pts], 255)
    return mask

def compute_speed(track_history_person, fps):
    """ì‹¤ì œ ë¹„ìœ¨ì„ ë°˜ì˜í•œ ì†ë„ ê³„ì‚°"""
    if len(track_history_person) < SPEED_WINDOW + 1:
        return 0.0
    p0 = track_history_person[0]
    p1 = track_history_person[SPEED_WINDOW]
    dt = (p1[0] - p0[0]) / fps if fps > 0 else 0.0
    if dt <= 0:
        return 0.0
    # í”½ì…€ ì°¨ì´
    dx = p1[1] - p0[1]  # xë°©í–¥ (ê°€ë¡œ)
    dy = p1[2] - p0[2]  # yë°©í–¥ (ì„¸ë¡œ)
    # í”½ì…€ â†’ ì‹¤ì œ ê±°ë¦¬(mm)
    scale_x = 7.89   # ê°€ë¡œ 1px = ì•½ 7.89 mm ê¸°ì¤€ : ì‹œê°ë³´ì¡° ë³´ë„ë¸”ëŸ­ ê°€ë¡œ ê¸¸ì´ 30cmê°€ì •
    scale_y = 44.6   # ì„¸ë¡œ 1px = ì•½ 44.6 mm ê¸°ì¤€ : íš¡ë‹¨ë³´ë„ ì„¸ë¡œê¸¸ì´ 0.5m * 4 + ì‚¬ì´ í­ 0.5m * 3
    dx_mm = dx * scale_x
    dy_mm = dy * scale_y
    # ì‹¤ì œ ê±°ë¦¬ (mm ë‹¨ìœ„)
    dist_mm = (dx_mm**2 + dy_mm**2) ** 0.5
    # ì†ë„ = ê±°ë¦¬ / ì‹œê°„ (m/së¡œ ë³€í™˜)
    return (dist_mm / 1000.0) / dt

def detect_speed_events(detections, track_history, fps):
    """ì†ë„ ê°ì§€ í•¨ìˆ˜"""
    events = []
    for detection in detections:
        person_id = detection['id']
        center = detection['center']
        zone = detection.get('zone', 'general')
        
        if person_id >= 0 and person_id in track_history:
            speed = compute_speed(track_history[person_id], fps)
            if speed > SPEED_THRESHOLD_GENERAL:
                event_detections["all_event_count"] += 1
                event_detections["all_event_persons"].add(person_id)
                events.append({
                    'type': 'speed_event',
                    'person_id': person_id,
                    'speed': speed,
                    'center': center,
                    'zone': zone
                })
    return events

def generate_frames(camera_id):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"ì˜ìƒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    frame_skip = 10  # í”„ë ˆì„ ê±´ë„ˆë›°ê¸°ë¥¼ ëŠ˜ë ¤ì„œ ì†ë„ ì¡°ì ˆ
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            continue

        frame_count += 1
        if frame_count % frame_skip != 0:
            continue

        # CAM2 - ì´ë²¤íŠ¸ ê°ì§€ í™”ë©´ (í•µì‹¬ ë¡œì§)
        display_frame = frame.copy()
        
        # ROI ì˜ì—­ ê·¸ë¦¬ê¸°
        for zone_name, coords in cam2_zone_coords.items():
            pts = np.array(coords, np.int32)
            cv2.polylines(display_frame, [pts], True, (0, 255, 255), 3)  # ë…¸ë€ìƒ‰
            cv2.putText(display_frame, "RUNNING ZONE", (coords[0][0], coords[0][1]-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # ì´ë²¤íŠ¸ ê°ì§€ ìˆ˜í–‰
        all_detections = []
        mask = create_roi_mask(frame.copy(), cam2_zone_coords["running_zone"])
        roi_frame = cv2.bitwise_and(frame, frame, mask=mask)
        
        # ëª¨ë¸ ë¡œë”© ì‹œë„
        try:
            m = get_model()
            if m is not None:
                # ì‚¬ëŒ ê°ì§€ ë° íŠ¸ë˜í‚¹
                results = m.track(
                    roi_frame,
                    persist=True,
                    conf=0.40,
                    iou=0.10,
                    classes=[0],
                    tracker="bytetrack.yaml",
                    verbose=False
                )
            else:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í”„ë ˆì„ ì‚¬ìš©
                results = None
        except Exception as e:
            print(f"ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            results = None
            
        if results is not None and results[0].boxes is not None and len(results[0].boxes) > 0:
            ids = results[0].boxes.id
            for i, (box, conf) in enumerate(zip(results[0].boxes.xyxy, results[0].boxes.conf)):
                x1, y1, x2, y2 = map(int, box)
                
                if is_valid_person_detection(x1, y1, x2, y2, conf):
                    tid = int(ids[i]) if ids is not None else -1
                    
                    # ì¶”ì  ì´ë ¥ ì—…ë°ì´íŠ¸
                    if tid >= 0:
                        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
                        w_box, h_box = (x2 - x1), (y2 - y1)
                        track_history[tid].append((frame_count, cx, cy, w_box, h_box))
                    
                    # ê°ì§€ ì •ë³´ ì €ì¥
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)
                    detection = {
                        'id': tid,
                        'bbox': (x1, y1, x2, y2),
                        'center': center,
                        'confidence': float(conf),
                        'zone': 'running_zone'
                    }
                    all_detections.append(detection)
        
        # ì´ë²¤íŠ¸ ê°ì§€ (ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ)
        events = detect_speed_events(all_detections, track_history, fps) if results is not None else []
        
        # YOLO ê²°ê³¼ ì‹œê°í™”
        if results is not None and results[0].boxes is not None:
            display_frame = results[0].plot(
                img=display_frame,
                labels=True,
                conf=False,
                probs=False,
                line_width=3
            )
        
        # ì´ë²¤íŠ¸ ì‹œê°í™”
        for event in events:
            if event['type'] == 'speed_event':
                center = event['center']
                speed = event['speed']
                person_id = event['person_id']
                
                # ì†ë„ ì´ë²¤íŠ¸ í‘œì‹œ
                cv2.circle(display_frame, center, 22, (0, 0, 255), 3)
                cv2.putText(display_frame, f"SPEED: {speed:.1f}m/s", 
                           (center[0]-45, center[1]-30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 4)
                
                # ê¶¤ì  ê·¸ë¦¬ê¸°
                if person_id >= 0 and person_id in track_history:
                    if len(track_history[person_id]) >= TRAIL_LEN:
                        pts = [(hx, hy) for _, hx, hy, _, _ in list(track_history[person_id])[-TRAIL_LEN:]]
                        for j in range(1, len(pts)):
                            cv2.line(display_frame, pts[j-1], pts[j], (255, 255, 0), 2)
        
        img = display_frame
        
        _, buffer = cv2.imencode('.jpg', img)
        frame_bytes = buffer.tobytes()

        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
        
        # í”„ë ˆì„ ê°„ê²© ì¡°ì ˆë¡œ ë¹„ë””ì˜¤ ì†ë„ ì¡°ì ˆ
        time.sleep(0.05)  # 50ms ì§€ì—° ì¶”ê°€

    cap.release()


# ------------------ Flask Routes ------------------
@cam2_bp.route('/')
def index():
    return render_template('cam2.html', zone_coords=cam2_zone_coords)

@cam2_bp.route('/camera/<camera_id>')
def camera_feed(camera_id):
    print(f"ì¹´ë©”ë¼ ìš”ì²­: {camera_id} (íƒ€ì…: {type(camera_id)})")
    return Response(generate_frames(camera_id),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

# ì•„ë˜ APIë“¤ì€ ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ (ì‹¤ì œë¡œëŠ” CAM2ì—ì„œ ì‚¬ìš© ì•ˆí•¨)
@cam2_bp.route('/api/zone_detections')
def get_zone_detections():
    with detection_lock:
        return jsonify({
            'zones': zone_detections,
            'total_detections': sum(zone_detections.values()),
            'danger_zones': [zone_id for zone_id, count in zone_detections.items() if count >= DANGER_THRESHOLD],
            'thresholds': {
                'warning': WARNING_THRESHOLD,
                'danger': DANGER_THRESHOLD,
            }
        })

@cam2_bp.route('/api/detection_stats')
def detection_stats():
    with detection_lock:
        return jsonify({
            'total_detections': sum(zone_detections.values()),
            'active_cameras': 5,  # MAIN + 4ê°œ ì¹´ë©”ë¼
            'zone_detections': zone_detections,
            'thresholds': {
                'warning': WARNING_THRESHOLD,
                'danger': DANGER_THRESHOLD,
            }
        })

@cam2_bp.route('/api/alerts')
def get_alerts():
    with detection_lock:
        alerts = []
        for zone_id, count in zone_detections.items():
            if count >= DANGER_THRESHOLD:
                alerts.append(f"ğŸš¨ ZONE {zone_id}: {count}ëª… íƒì§€ - ìœ„í—˜ ìˆ˜ì¤€!")
            elif count >= WARNING_THRESHOLD:
                alerts.append(f"âš¡ ZONE {zone_id}: {count}ëª… íƒì§€ - ì£¼ì˜ í•„ìš”")
        
        if not alerts:
            alerts.append("âœ… ëª¨ë“  êµ¬ì—­ ì •ìƒ ìƒíƒœ")
            
        return jsonify({'alerts': alerts})


# ==============================================================================
# --- í”„ë ˆì„ í¬ê¸° ìºì‹œ ë° API ---
_FRAME_WH = [None, None]

def _compute_frame_wh():
    if _FRAME_WH[0] and _FRAME_WH[1]:
        return _FRAME_WH[0], _FRAME_WH[1]
    try:
        # CAM2 ì˜ìƒìœ¼ë¡œ í¬ê¸° íšë“
        cap = cv2.VideoCapture(video_path)
        ret, fr = cap.read()
        cap.release()
        if ret:
            H, W = fr.shape[:2]
            _FRAME_WH[0], _FRAME_WH[1] = W, H
            return W, H
    except:
        pass
    return 1920, 1080

@cam2_bp.get("/api/frame_wh")
def api_frame_wh():
    W, H = _compute_frame_wh()
    return jsonify({"width": int(W), "height": int(H)})

# --- R í‚¤ë¡œ /roi íŒì—… ì—´ê¸° ---
@cam2_bp.get("/hotkeys.js")
def hotkeys_js():
    js = r"""
(()=>{function openPicker(){
  try{
    const w=window.open('/cam2/roi','roi','width=1280,height=820,noopener,noreferrer');
    if(!w) return;
  }catch(e){}
}
window.addEventListener('keydown',(e)=>{
  if(e.repeat) return;
  if(e.key==='r'||e.key==='R') openPicker();
});})();
"""
    return Response(js, mimetype="application/javascript")

# --- HTML ì‘ë‹µì— hotkeys.js ìë™ ì‚½ì… ---
@cam2_bp.after_request
def inject_hotkeys(resp):
    try:
        ctype = resp.headers.get('Content-Type','')
        if 'text/html' in ctype.lower():
            body = resp.get_data(as_text=True)
            if ('/cam2/hotkeys.js' not in body) and ('</body>' in body):
                body = body.replace('</body>', '<script src="/cam2/hotkeys.js"></script></body>')
                resp.set_data(body)
    except Exception as e:
        print('[inject_hotkeys] skip:', e)
    return resp

# --- ROI í´ë¦­ íŒì—… í˜ì´ì§€ ---
@cam2_bp.get("/roi")
def roi_page():
    return render_template('roi_piker.html')

@cam2_bp.post("/api/roi/<int:zid>")
def api_set_roi(zid:int):
    data = request.get_json(silent=True) or {}
    pts = data.get("points", [])
    if not isinstance(pts, list) or len(pts) < 4:
        return jsonify({"ok": False, "error": "at least 4 points required"}), 400

    # í”„ë ˆì„ í¬ê¸°
    W, H = _compute_frame_wh()
    def clamp(v, lo, hi):
        v = int(round(float(v)))
        return max(lo, min(hi, v))

    xy = []
    try:
        for p in pts:
            x = clamp(p["x"], 0, W-1)
            y = clamp(p["y"], 0, H-1)
            xy.append((x, y))
    except Exception:
        return jsonify({"ok": False, "error": "invalid point format"}), 400

    # CAM2 ëŸ¬ë‹ì¡´ì— ë¬´ì¡°ê±´ ì €ì¥ (zidëŠ” UIìš©)
    cam2_zone_coords["running_zone"] = xy

    # (ì„ íƒ) ë©”ì¸ ZONE overlayë„ í•¨ê»˜ ê°±ì‹ í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ì¤„ì„ ì¼œì„¸ìš”.
    # zone_coords[str(zid)] = xy

    return jsonify({"ok": True, "zone": zid, "num_points": len(xy)})

@cam2_bp.get("/api/events_total")
def api_events_total():
    return jsonify({
        "events_total": len(event_detections["all_event_persons"])
    })

# @cam2_bp.get("/api/current_tracking_count")
# def get_current_tracking_count():
#     # ì¶”ì  ì¤‘ì¸ ì‚¬ëŒ ìˆ˜ ë°˜í™˜
#     return jsonify({
#         'tracking_count': len(event_detections["all_event_persons"])
#     })